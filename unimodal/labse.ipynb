{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10888437,"sourceType":"datasetVersion","datasetId":6766066}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:59:10.769663Z","iopub.execute_input":"2025-03-01T10:59:10.769946Z","iopub.status.idle":"2025-03-01T10:59:10.774442Z","shell.execute_reply.started":"2025-03-01T10:59:10.769924Z","shell.execute_reply":"2025-03-01T10:59:10.773440Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load the dataset\ndataset_path = \"/kaggle/input/dataset-text/TELUGU_METADATA.xlsx\"  # Update the path as per your dataset location\ndata = pd.read_excel(dataset_path,sheet_name=\"BinaryClass\")\n\n# Display the first few rows of the dataset\nprint(data.head())\n# Check the shape of the dataset\nprint(f\"Dataset shape: {data.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T12:08:27.687883Z","iopub.execute_input":"2025-03-01T12:08:27.688189Z","iopub.status.idle":"2025-03-01T12:08:27.760897Z","shell.execute_reply.started":"2025-03-01T12:08:27.688154Z","shell.execute_reply":"2025-03-01T12:08:27.759959Z"}},"outputs":[{"name":"stdout","text":"        AUDIO FILE NAME CLASS LABLE  \\\n0  H_TE_001_R_F_001_001           H   \n1  H_TE_001_R_F_001_002           H   \n2  H_TE_001_R_M_001_003           H   \n3  H_TE_001_R_M_001_004           H   \n4  H_TE_001_R_M_001_005           H   \n\n                                       TRANSCRIPTION  \n0               ఎస్సీలుగా పుట్టాలని ఎవరు కోరుకుంటారు  \n1  ఎవరు మాత్రం SC కులంలో పుట్టాలని కోరుకుంటారు అం...  \n2        ఎవరు మాత్రం SC కులంలో పుట్టాలని కోరుకుంటారు  \n3  ఎవరు మాత్రం SC కులంలో పుట్టాలని కోరుకుంటారు డబ...  \n4  అందరూ రాజుల కులంలో పుడితే రాజ్యాన్ని ఎలచ్చనుకు...  \nDataset shape: (601, 3)\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"# Drop unnecessary columns and keep only relevant ones\ndata = data[['AUDIO FILE NAME', 'CLASS LABLE', 'TRANSCRIPTION']]\n\n# Map labels to binary values (Hate: 1, Non-Hate: 0)\ndata['CLASS LABLE'] = data['CLASS LABLE'].map({'H': 1, 'NH': 0})\n\n# Check for missing values\nprint(data.isnull().sum())\n\n# Drop rows with missing values (if any)\ndata = data.dropna()\n\n# Display the cleaned dataset\nprint(data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:01:52.795295Z","iopub.execute_input":"2025-03-01T11:01:52.795752Z","iopub.status.idle":"2025-03-01T11:01:52.815120Z","shell.execute_reply.started":"2025-03-01T11:01:52.795724Z","shell.execute_reply":"2025-03-01T11:01:52.814310Z"}},"outputs":[{"name":"stdout","text":"AUDIO FILE NAME    0\nCLASS LABLE        0\nTRANSCRIPTION      0\ndtype: int64\n        AUDIO FILE NAME  CLASS LABLE  \\\n0  H_TE_001_R_F_001_001            1   \n1  H_TE_001_R_F_001_002            1   \n2  H_TE_001_R_M_001_003            1   \n3  H_TE_001_R_M_001_004            1   \n4  H_TE_001_R_M_001_005            1   \n\n                                       TRANSCRIPTION  \n0               ఎస్సీలుగా పుట్టాలని ఎవరు కోరుకుంటారు  \n1  ఎవరు మాత్రం SC కులంలో పుట్టాలని కోరుకుంటారు అం...  \n2        ఎవరు మాత్రం SC కులంలో పుట్టాలని కోరుకుంటారు  \n3  ఎవరు మాత్రం SC కులంలో పుట్టాలని కోరుకుంటారు డబ...  \n4  అందరూ రాజుల కులంలో పుడితే రాజ్యాన్ని ఎలచ్చనుకు...  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Define a custom Dataset class\nclass HateSpeechDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Set up device (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:03:27.514591Z","iopub.execute_input":"2025-03-01T11:03:27.514945Z","iopub.status.idle":"2025-03-01T11:03:27.574660Z","shell.execute_reply.started":"2025-03-01T11:03:27.514916Z","shell.execute_reply":"2025-03-01T11:03:27.573840Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Split the data into training and testing sets\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Load the LaBSE tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n\n# Define parameters\nMAX_LEN = 128\nBATCH_SIZE = 16\n\n# Create datasets\ntrain_dataset = HateSpeechDataset(train_data['TRANSCRIPTION'].tolist(), train_data['CLASS LABLE'].tolist(), tokenizer, MAX_LEN)\ntest_dataset = HateSpeechDataset(test_data['TRANSCRIPTION'].tolist(), test_data['CLASS LABLE'].tolist(), tokenizer, MAX_LEN)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:28:15.085901Z","iopub.execute_input":"2025-03-01T11:28:15.086209Z","iopub.status.idle":"2025-03-01T11:28:16.602945Z","shell.execute_reply.started":"2025-03-01T11:28:15.086184Z","shell.execute_reply":"2025-03-01T11:28:16.601971Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Load the LaBSE model\nmodel = AutoModel.from_pretrained(\"sentence-transformers/LaBSE\").to(device)\n\n# Add a classification head\nclass HateSpeechClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(HateSpeechClassifier, self).__init__()\n        self.labse = model\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.labse.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.labse(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        output = self.drop(pooled_output)\n        return self.out(output)\n\n# Initialize the model\nmodel = HateSpeechClassifier(n_classes=2).to(device)\n\n# Define optimizer and loss function\noptimizer = optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = nn.CrossEntropyLoss().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:31:44.405702Z","iopub.execute_input":"2025-03-01T11:31:44.406047Z","iopub.status.idle":"2025-03-01T11:31:45.154764Z","shell.execute_reply.started":"2025-03-01T11:31:44.406017Z","shell.execute_reply":"2025-03-01T11:31:45.154057Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Training function\ndef train_epoch(model, data_loader, loss_fn, optimizer, device):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n\n    for d in data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        labels = d[\"label\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, labels)\n\n        correct_predictions += torch.sum(preds == labels)\n        losses.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n\n# Training loop\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    train_acc, train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device)\n    print(f'Train loss: {train_loss}, Train accuracy: {train_acc}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:31:45.806364Z","iopub.execute_input":"2025-03-01T11:31:45.806684Z","iopub.status.idle":"2025-03-01T11:34:16.244378Z","shell.execute_reply.started":"2025-03-01T11:31:45.806660Z","shell.execute_reply":"2025-03-01T11:34:16.243591Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\nTrain loss: 0.5754983445008596, Train accuracy: 0.6791666666666667\nEpoch 2/20\nTrain loss: 0.38521497597297033, Train accuracy: 0.8270833333333333\nEpoch 3/20\nTrain loss: 0.24501242687304814, Train accuracy: 0.91875\nEpoch 4/20\nTrain loss: 0.13579967767000198, Train accuracy: 0.9604166666666667\nEpoch 5/20\nTrain loss: 0.0588349886238575, Train accuracy: 0.9833333333333333\nEpoch 6/20\nTrain loss: 0.01972611063780884, Train accuracy: 0.9958333333333333\nEpoch 7/20\nTrain loss: 0.00938042439520359, Train accuracy: 1.0\nEpoch 8/20\nTrain loss: 0.005005022763119389, Train accuracy: 1.0\nEpoch 9/20\nTrain loss: 0.003037321373509864, Train accuracy: 1.0\nEpoch 10/20\nTrain loss: 0.0028546103955401727, Train accuracy: 1.0\nEpoch 11/20\nTrain loss: 0.0017946678349593033, Train accuracy: 1.0\nEpoch 12/20\nTrain loss: 0.0014349457924254239, Train accuracy: 1.0\nEpoch 13/20\nTrain loss: 0.001425603588965411, Train accuracy: 1.0\nEpoch 14/20\nTrain loss: 0.0028009679246072967, Train accuracy: 1.0\nEpoch 15/20\nTrain loss: 0.08148963725349555, Train accuracy: 0.9770833333333333\nEpoch 16/20\nTrain loss: 0.030295558196182053, Train accuracy: 0.9854166666666666\nEpoch 17/20\nTrain loss: 0.005129148423050841, Train accuracy: 1.0\nEpoch 18/20\nTrain loss: 0.0027135468553751707, Train accuracy: 1.0\nEpoch 19/20\nTrain loss: 0.002062070940155536, Train accuracy: 1.0\nEpoch 20/20\nTrain loss: 0.001713748877712836, Train accuracy: 1.0\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Evaluation function\ndef eval_model(model, data_loader, loss_fn, device):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            labels = d[\"label\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, labels)\n\n            correct_predictions += torch.sum(preds == labels)\n            losses.append(loss.item())\n\n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n\n# Evaluate the model\ntest_acc, test_loss = eval_model(model, test_loader, loss_fn, device)\nprint(f'Test loss: {test_loss}, Test accuracy: {test_acc}')\n\n# Generate classification report\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for d in test_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        labels = d[\"label\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(preds.cpu().numpy())\n\nprint(classification_report(y_true, y_pred, target_names=['Non-Hate', 'Hate']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:34:16.245995Z","iopub.execute_input":"2025-03-01T11:34:16.246225Z","iopub.status.idle":"2025-03-01T11:34:17.197554Z","shell.execute_reply.started":"2025-03-01T11:34:16.246206Z","shell.execute_reply":"2025-03-01T11:34:17.196628Z"}},"outputs":[{"name":"stdout","text":"Test loss: 0.48502410924993455, Test accuracy: 0.8925619834710744\n              precision    recall  f1-score   support\n\n    Non-Hate       0.87      0.81      0.84        42\n        Hate       0.90      0.94      0.92        79\n\n    accuracy                           0.89       121\n   macro avg       0.89      0.87      0.88       121\nweighted avg       0.89      0.89      0.89       121\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"**MULTICLASS**","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T14:31:33.155023Z","iopub.execute_input":"2025-03-01T14:31:33.155311Z","iopub.status.idle":"2025-03-01T14:31:33.159332Z","shell.execute_reply.started":"2025-03-01T14:31:33.155282Z","shell.execute_reply":"2025-03-01T14:31:33.158526Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load the dataset\ndataset_path = \"/kaggle/input/dataset-text/TELUGU_METADATA.xlsx\"  # Update the path as per your dataset location\ndata = pd.read_excel(dataset_path,sheet_name=\"MultiClass\")\n\n# Display the first few rows of the dataset\nprint(data.head())\n# Check the shape of the dataset\nprint(f\"Dataset shape: {data.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T14:31:36.031180Z","iopub.execute_input":"2025-03-01T14:31:36.031899Z","iopub.status.idle":"2025-03-01T14:31:36.567663Z","shell.execute_reply.started":"2025-03-01T14:31:36.031852Z","shell.execute_reply":"2025-03-01T14:31:36.566698Z"}},"outputs":[{"name":"stdout","text":"        AUDIO FILE NAME CLASS LABLE  \\\n0  H_TE_001_R_F_001_001           R   \n1  H_TE_001_R_F_001_002           R   \n2  H_TE_001_R_M_001_003           R   \n3  H_TE_001_R_M_001_004           R   \n4  H_TE_001_R_M_001_005           R   \n\n                                                TEXT  \n0               ఎస్సీలుగా పుట్టాలని ఎవరు కోరుకుంటారు  \n1  ఎవరు మాత్రం SC కులంలో పుట్టాలని కోరుకుంటారు అం...  \n2        ఎవరు మాత్రం SC కులంలో పుట్టాలని కోరుకుంటారు  \n3  ఎవరు మాత్రం SC కులంలో పుట్టాలని కోరుకుంటారు డబ...  \n4  అందరూ రాజుల కులంలో పుడితే రాజ్యాన్ని ఎలచ్చనుకు...  \nDataset shape: (601, 3)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Define the label mapping\nlabel_mapping = {\n    'NH': 0,  # Non-Hate\n    'C': 1,   # Character\n    'G': 2,   # Gender\n    'R': 3,   # Religion\n    'P': 4    # Political\n}\n\n# Apply the mapping to the CLASS LABLE column\ndata['CLASS LABLE'] = data['CLASS LABLE'].map(label_mapping)\n\n# Check the updated class distribution\nprint(data['CLASS LABLE'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T14:31:39.094595Z","iopub.execute_input":"2025-03-01T14:31:39.095240Z","iopub.status.idle":"2025-03-01T14:31:39.112845Z","shell.execute_reply.started":"2025-03-01T14:31:39.095201Z","shell.execute_reply":"2025-03-01T14:31:39.111843Z"}},"outputs":[{"name":"stdout","text":"CLASS LABLE\n0    208\n1    132\n2    111\n3     82\n4     68\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Drop rows with missing or invalid labels\ndata = data.dropna(subset=['CLASS LABLE'])\n\n# Check the cleaned dataset\nprint(data['CLASS LABLE'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T14:31:42.962164Z","iopub.execute_input":"2025-03-01T14:31:42.962490Z","iopub.status.idle":"2025-03-01T14:31:42.972312Z","shell.execute_reply.started":"2025-03-01T14:31:42.962447Z","shell.execute_reply":"2025-03-01T14:31:42.971401Z"}},"outputs":[{"name":"stdout","text":"CLASS LABLE\n0    208\n1    132\n2    111\n3     82\n4     68\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Define a custom Dataset class\nclass HateSpeechDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Set up device (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T14:31:45.006093Z","iopub.execute_input":"2025-03-01T14:31:45.006425Z","iopub.status.idle":"2025-03-01T14:31:45.062777Z","shell.execute_reply.started":"2025-03-01T14:31:45.006397Z","shell.execute_reply":"2025-03-01T14:31:45.061968Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Split the data into training and testing sets\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Load the LaBSE tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n\n# Define parameters\nMAX_LEN = 128\nBATCH_SIZE = 8\n\n# Create datasets\ntrain_dataset = HateSpeechDataset(train_data['TEXT'].tolist(), train_data['CLASS LABLE'].tolist(), tokenizer, MAX_LEN)\ntest_dataset = HateSpeechDataset(test_data['TEXT'].tolist(), test_data['CLASS LABLE'].tolist(), tokenizer, MAX_LEN)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T14:31:50.318125Z","iopub.execute_input":"2025-03-01T14:31:50.318414Z","iopub.status.idle":"2025-03-01T14:31:53.452718Z","shell.execute_reply.started":"2025-03-01T14:31:50.318393Z","shell.execute_reply":"2025-03-01T14:31:53.452024Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/397 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b1ec227577c487699b98ae6dbfcba50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/804 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beb109c3c7264b85b9dc7ecd0120bead"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/5.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de39125f0ed495495c1d09e588954d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.62M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"569b0ed271794cd5bf47377552b9acdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5caf1974ced4885bf83793494384b83"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Load the LaBSE model\nmodel = AutoModel.from_pretrained(\"sentence-transformers/LaBSE\").to(device)\n\n# Add a classification head\nclass HateSpeechClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(HateSpeechClassifier, self).__init__()\n        self.labse = model\n        self.drop = nn.Dropout(p=0.4)\n        self.out = nn.Linear(self.labse.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.labse(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        output = self.drop(pooled_output)\n        return self.out(output)\n\n# Initialize the model\nmodel = HateSpeechClassifier(n_classes=5).to(device)  # 5 classes: Gender, Religion, Character, Political, Non-Hate\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\nloss_fn = nn.CrossEntropyLoss().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T14:40:40.525987Z","iopub.execute_input":"2025-03-01T14:40:40.526344Z","iopub.status.idle":"2025-03-01T14:40:41.458098Z","shell.execute_reply.started":"2025-03-01T14:40:40.526316Z","shell.execute_reply":"2025-03-01T14:40:41.457393Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Training function\ndef train_epoch(model, data_loader, loss_fn, optimizer, device):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n\n    for d in data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        labels = d[\"label\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, labels)\n\n        correct_predictions += torch.sum(preds == labels)\n        losses.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n\n# Training loop\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    train_acc, train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device)\n    print(f'Train loss: {train_loss}, Train accuracy: {train_acc}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T14:41:11.445740Z","iopub.execute_input":"2025-03-01T14:41:11.446046Z","iopub.status.idle":"2025-03-01T14:42:54.575100Z","shell.execute_reply.started":"2025-03-01T14:41:11.446024Z","shell.execute_reply":"2025-03-01T14:42:54.574180Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\nTrain loss: 1.265400559703509, Train accuracy: 0.5041666666666667\nEpoch 2/10\nTrain loss: 0.62682274132967, Train accuracy: 0.7791666666666667\nEpoch 3/10\nTrain loss: 0.2638416544223825, Train accuracy: 0.9270833333333334\nEpoch 4/10\nTrain loss: 0.09579355257252851, Train accuracy: 0.9916666666666667\nEpoch 5/10\nTrain loss: 0.02927128605855008, Train accuracy: 1.0\nEpoch 6/10\nTrain loss: 0.01281562818524738, Train accuracy: 1.0\nEpoch 7/10\nTrain loss: 0.008377978399706384, Train accuracy: 1.0\nEpoch 8/10\nTrain loss: 0.006358478311449289, Train accuracy: 1.0\nEpoch 9/10\nTrain loss: 0.005048331358314802, Train accuracy: 1.0\nEpoch 10/10\nTrain loss: 0.0041293374573191, Train accuracy: 1.0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Evaluation function\ndef eval_model(model, data_loader, loss_fn, device):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            labels = d[\"label\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, labels)\n\n            correct_predictions += torch.sum(preds == labels)\n            losses.append(loss.item())\n\n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n\n# Evaluate the model\ntest_acc, test_loss = eval_model(model, test_loader, loss_fn, device)\nprint(f'Test loss: {test_loss}, Test accuracy: {test_acc}')\n\n# Generate classification report\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for d in test_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        labels = d[\"label\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(preds.cpu().numpy())\n\n# Define target names for multi-class classification\ntarget_names = ['Gender', 'Religion', 'Character', 'Political', 'Non-Hate']\nprint(classification_report(y_true, y_pred, target_names=target_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T14:38:22.757995Z","iopub.execute_input":"2025-03-01T14:38:22.758225Z","iopub.status.idle":"2025-03-01T14:38:23.808427Z","shell.execute_reply.started":"2025-03-01T14:38:22.758205Z","shell.execute_reply":"2025-03-01T14:38:23.807531Z"}},"outputs":[{"name":"stdout","text":"Test loss: 1.1372189662251913, Test accuracy: 0.7603305785123967\n              precision    recall  f1-score   support\n\n      Gender       0.83      0.81      0.82        42\n    Religion       0.67      0.73      0.70        22\n   Character       0.64      0.70      0.67        23\n   Political       0.79      0.58      0.67        19\n    Non-Hate       0.88      1.00      0.94        15\n\n    accuracy                           0.76       121\n   macro avg       0.76      0.76      0.76       121\nweighted avg       0.76      0.76      0.76       121\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}